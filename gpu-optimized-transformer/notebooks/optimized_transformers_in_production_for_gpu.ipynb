{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html ipywidgets pytorch-quantization\n",
    "# !pip3 install git+https://github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\&subdirectory=tools/pytorch-quantization/==8.2.2\n",
    "# !pip3 install git+https://github.com/ELS-RD/transformer-deploy\n",
    "# !pip3 install datasets sklearn seaborn onnxruntime-gpu==1.9.0 pycuda nvidia-tensorrt==8.2.2.1\n",
    "# !pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize transformers in production on GPU üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù Note: This notebook covers everything you need to know about optimizing a particular transformer model for production on GPU. The knowledge that is used here comes from the TensorRT documentation, ONNX documentation and the transformers-deploy library (ELS-RD) documentation + end-to-end quantization notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code snippets in this notebook come from the [end-to-end quantization notebook](https://github.com/ELS-RD/transformer-deploy/blob/main/demo/quantization/quantization_end_to_end.ipynb) of the transformers-deploy library. This library has some interesting insights + examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used materials:\n",
    "- Model used in this notebook: [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "- Dataset for benchmarking: [SST2 GLUE dataset](https://huggingface.co/datasets/glue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ó HuggingFace provides a wide range of models and datasets that can improve a lot of applications. Be sure to check their website: https://huggingface.co/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 31 13:54:42 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 511.23       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:2D:00.0  On |                  N/A |\n",
      "|  0%   58C    P8    24W / 270W |   1451MiB /  8192MiB |     17%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "import transformers\n",
    "from pycuda._driver import Stream\n",
    "from tensorrt.tensorrt import IExecutionContext, Logger, Runtime\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    IntervalStrategy,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, load_metric\n",
    "from collections import OrderedDict\n",
    "\n",
    "from transformer_deploy.backends.ort_utils import (\n",
    "    cpu_quantization,\n",
    "    create_model_for_provider,\n",
    "    optimize_onnx,\n",
    ")\n",
    "from transformer_deploy.backends.pytorch_utils import convert_to_onnx, get_model_size\n",
    "from transformer_deploy.backends.trt_utils import build_engine, get_binding_idxs, infer_tensorrt, load_engine\n",
    "from transformer_deploy.benchmarks.utils import print_timings, track_infer_time\n",
    "from transformer_deploy.QDQModels.calibration_utils import QATCalibrate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "from transformer_deploy.backends.trt_utils import build_engine, load_engine, save_engine\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the logging level to error to improve intelligibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = logging.ERROR\n",
    "logging.getLogger().setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "trt_logger: Logger = trt.Logger(trt.Logger.ERROR)\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some standard variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93832ddea4154a9dab2e4f1b0cada15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task = \"sst2\"\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "padding = \"max_length\"\n",
    "labels_mapping = {\"negative\": 0, \"positive\": 1 }\n",
    "accuracy_score = load_metric('accuracy')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sentiment_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "columns = [\"name\", \"average_latency\", \"std_latency\", \"accuracy\", \"size\"]\n",
    "benchmark_results_df = pd.DataFrame(columns=columns)\n",
    "validation_key = \"validation\"\n",
    "num_labels = 2\n",
    "batch_size = 32\n",
    "max_seq_length = 128\n",
    "columns = [\"name\", \"average_latency\", \"std_latency\", \"accuracy\", \"size\"]\n",
    "benchmark_results_df = pd.DataFrame(columns=columns)\n",
    "temp_dir = \"./temp\"\n",
    "model_path = f\"{temp_dir}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_steps = 1000\n",
    "strategy = IntervalStrategy.STEPS\n",
    "args = TrainingArguments(\n",
    "    f\"{temp_dir}/{model_name}\",\n",
    "    evaluation_strategy=strategy,\n",
    "    eval_steps=nr_steps,\n",
    "    logging_steps=nr_steps,\n",
    "    save_steps=nr_steps,\n",
    "    save_strategy=strategy,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size * 2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    group_by_length=True,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2id(label):\n",
    "    return labels_mapping.get(label, None)\n",
    "\n",
    "def preprocess(data):\n",
    "    return tokenizer(data[\"sentence\"], truncation=True, padding=\"max_length\", max_length=max_seq_length)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def convert_tensor(data, output: str):\n",
    "    input = OrderedDict()\n",
    "    for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if k in data:\n",
    "            v = data[k]\n",
    "            if output == \"torch\":\n",
    "                value = torch.tensor(v, dtype=torch.long, device=\"cuda\")\n",
    "            elif output == \"np\":\n",
    "                value = np.asarray(v, dtype=np.int32)\n",
    "            else:\n",
    "                raise Exception(f\"unknown output type: {output}\")\n",
    "            input[k] = value\n",
    "    return input\n",
    "\n",
    "\n",
    "def measure_accuracy(infer, int64: bool) -> float:\n",
    "    outputs = list()\n",
    "    for start_index in range(0, len(encoded_dataset[validation_key]), batch_size):\n",
    "        end_index = start_index + batch_size\n",
    "        data = encoded_dataset[validation_key][start_index:end_index]\n",
    "        inputs = convert_tensor(data=data, output=\"np\")\n",
    "        if int64:\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.astype(np.int64)\n",
    "        output = infer(inputs)\n",
    "        output = np.argmax(output[0], axis=1).astype(int).tolist()\n",
    "        outputs.extend(output)\n",
    "    return np.mean(np.array(outputs) == np.array(validation_labels))\n",
    "\n",
    "\n",
    "def get_trainer(model: PreTrainedModel) -> Trainer:\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[validation_key],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e2f37933ed4f458d966baec0704f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sentiment_dataset = load_dataset(\"glue\", task)\n",
    "metric = load_metric(\"glue\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = sentiment_dataset.map(preprocess, batched=True)\n",
    "columns_to_return = ['input_ids', 'label', 'attention_mask']\n",
    "encoded_dataset.set_format(type='torch', columns=columns_to_return)\n",
    "validation_labels = [item[\"label\"] for item in encoded_dataset[validation_key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration:\n",
    "The intent of calibration during PTQ is to add QDQ nodes (Quantize-Dequantize) before and after operations to to compute their dynamic ranges. One node will contain the information to perform a mapping between high precision and lower precision number. This is really imporant process in static quantization, especially for the accuracy.  \n",
    "\n",
    "One of the important factors is the range we want to capture the different tensor values in. To find the best range you can use histrogram analysis (=try different percentiles to find the best range, the one with the highest accuracy is the best). If the percentile value is too small, too many values will be put outside the covered range. On the other hand if it's too big, the range will be very big. Both can lead to loss of granularity in model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28649/1028507753.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  k: torch.tensor(v, dtype=torch.long, device=\"cuda\")\n",
      "[INFO|trainer.py:440] 2022-01-31 13:01:31,044 >> Using amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentile: 99.9\n",
      "{'eval_loss': 0.4096519351005554, 'eval_accuracy': 0.9013761467889908, 'eval_runtime': 2.8337, 'eval_samples_per_second': 307.724, 'eval_steps_per_second': 4.941}\n",
      "{'eval_loss': 0.4096519351005554, 'eval_accuracy': 0.9013761467889908, 'eval_runtime': 2.8337, 'eval_samples_per_second': 307.724, 'eval_steps_per_second': 4.941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:440] 2022-01-31 13:02:16,777 >> Using amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentile: 99.99\n",
      "{'eval_loss': 0.3778071999549866, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 2.6232, 'eval_samples_per_second': 332.421, 'eval_steps_per_second': 5.337}\n",
      "{'eval_loss': 0.3778071999549866, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 2.6232, 'eval_samples_per_second': 332.421, 'eval_steps_per_second': 5.337}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:440] 2022-01-31 13:03:00,496 >> Using amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentile: 99.999\n",
      "{'eval_loss': 0.4061073064804077, 'eval_accuracy': 0.9025229357798165, 'eval_runtime': 2.523, 'eval_samples_per_second': 345.619, 'eval_steps_per_second': 5.549}\n",
      "{'eval_loss': 0.4061073064804077, 'eval_accuracy': 0.9025229357798165, 'eval_runtime': 2.523, 'eval_samples_per_second': 345.619, 'eval_steps_per_second': 5.549}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:440] 2022-01-31 13:03:44,752 >> Using amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentile: 99.9999\n",
      "{'eval_loss': 0.4007592797279358, 'eval_accuracy': 0.9025229357798165, 'eval_runtime': 2.586, 'eval_samples_per_second': 337.203, 'eval_steps_per_second': 5.414}\n",
      "{'eval_loss': 0.4007592797279358, 'eval_accuracy': 0.9025229357798165, 'eval_runtime': 2.586, 'eval_samples_per_second': 337.203, 'eval_steps_per_second': 5.414}\n"
     ]
    }
   ],
   "source": [
    "for percentile in [99.9, 99.99, 99.999, 99.9999]:\n",
    "    with QATCalibrate(method=\"histogram\", percentile=percentile) as qat:\n",
    "        model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_path, num_labels=num_labels\n",
    "        )\n",
    "        model_q = model_q.cuda()\n",
    "        qat.setup_model_qat(model_q)  # prepare quantizer to any model\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for start_index in range(0, 128, batch_size):\n",
    "                end_index = start_index + batch_size\n",
    "                data = encoded_dataset[\"train\"][start_index:end_index]\n",
    "                input_torch = {\n",
    "                    k: torch.tensor(v, dtype=torch.long, device=\"cuda\")\n",
    "                    for k, v in data.items()\n",
    "                    if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    "                }\n",
    "                model_q(**input_torch)\n",
    "    trainer = get_trainer(model_q)\n",
    "    print(f\"percentile: {percentile}\")\n",
    "    print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you found your range, use this for the rest the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28649/2923631624.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  k: torch.tensor(v, dtype=torch.long, device=\"cuda\")\n",
      "[INFO|trainer.py:440] 2022-01-31 13:04:41,447 >> Using amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3778071999549866, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 2.7206, 'eval_samples_per_second': 320.518, 'eval_steps_per_second': 5.146}\n",
      "{'eval_loss': 0.3778071999549866, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 2.7206, 'eval_samples_per_second': 320.518, 'eval_steps_per_second': 5.146}\n"
     ]
    }
   ],
   "source": [
    "with QATCalibrate(method=\"histogram\", percentile=99.99) as qat:\n",
    "    model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=num_labels\n",
    "    )\n",
    "    model_q = model_q.cuda()\n",
    "    qat.setup_model_qat(model_q)  # prepare quantizer to any model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start_index in range(0, 128, batch_size):\n",
    "            end_index = start_index + batch_size\n",
    "            data = encoded_dataset[\"train\"][start_index:end_index]\n",
    "            input_torch = {\n",
    "                k: torch.tensor(v, dtype=torch.long, device=\"cuda\")\n",
    "                for k, v in data.items()\n",
    "                if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    "            }\n",
    "            model_q(**input_torch)\n",
    "trainer = get_trainer(model_q)\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet you will enable quantization per one layer to see if one layer has a bigger accuracy cost than other layers. This is really important if you want to minimize the accuracy cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.0\n",
      "{'eval_loss': 0.37366706132888794, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 1.4657, 'eval_samples_per_second': 594.948, 'eval_steps_per_second': 9.552}\n",
      "----\n",
      "layer.1\n",
      "{'eval_loss': 0.37543460726737976, 'eval_accuracy': 0.9128440366972477, 'eval_runtime': 1.0704, 'eval_samples_per_second': 814.663, 'eval_steps_per_second': 13.079}\n",
      "----\n",
      "layer.2\n",
      "{'eval_loss': 0.40880557894706726, 'eval_accuracy': 0.8979357798165137, 'eval_runtime': 1.0281, 'eval_samples_per_second': 848.196, 'eval_steps_per_second': 13.618}\n",
      "----\n",
      "layer.3\n",
      "{'eval_loss': 0.3817985951900482, 'eval_accuracy': 0.9059633027522935, 'eval_runtime': 1.0321, 'eval_samples_per_second': 844.914, 'eval_steps_per_second': 13.565}\n",
      "----\n",
      "layer.4\n",
      "{'eval_loss': 0.39371609687805176, 'eval_accuracy': 0.9071100917431193, 'eval_runtime': 1.0108, 'eval_samples_per_second': 862.68, 'eval_steps_per_second': 13.85}\n",
      "----\n",
      "layer.5\n",
      "{'eval_loss': 0.38991713523864746, 'eval_accuracy': 0.908256880733945, 'eval_runtime': 1.006, 'eval_samples_per_second': 866.783, 'eval_steps_per_second': 13.916}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    layer_name = f\"layer.{i}\"\n",
    "    print(layer_name)\n",
    "    for name, module in model_q.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if layer_name in name:\n",
    "                module.enable_quant()\n",
    "            else:\n",
    "                module.disable_quant()\n",
    "    trainer.evaluate()\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of different operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as layer analysis, but for operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul\n",
      "{'eval_loss': 0.39034372568130493, 'eval_accuracy': 0.9071100917431193, 'eval_runtime': 1.474, 'eval_samples_per_second': 591.602, 'eval_steps_per_second': 9.498}\n",
      "----\n",
      "layernorm\n",
      "{'eval_loss': 0.390187531709671, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 0.9039, 'eval_samples_per_second': 964.725, 'eval_steps_per_second': 15.489}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for op in [\"matmul\", \"layernorm\"]:\n",
    "    for name, module in model_q.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if op in name:\n",
    "                module.enable_quant()\n",
    "            else:\n",
    "                module.disable_quant()\n",
    "    print(op)\n",
    "    trainer.evaluate()\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable certain layers if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3778071999549866, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 2.5536, 'eval_samples_per_second': 341.482, 'eval_steps_per_second': 5.483}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3778071999549866,\n",
       " 'eval_accuracy': 0.9105504587155964,\n",
       " 'eval_runtime': 2.5536,\n",
       " 'eval_samples_per_second': 341.482,\n",
       " 'eval_steps_per_second': 5.483}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disable_layer_names = []\n",
    "\n",
    "for name, module in model_q.named_modules():\n",
    "    if isinstance(module, quant_nn.TensorQuantizer):\n",
    "        if any([f\"{l}.output.layernorm\" in name for l in disable_layer_names]):\n",
    "            print(f\"disable {name}\")\n",
    "            module.disable_quant()\n",
    "        else:\n",
    "            module.enable_quant()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization Aware Training (QAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will retrain the calibrated model with 1/10 or 1/100 of the original learning rate. The goal is to retrieve most of the original accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:440] 2022-01-31 13:07:41,837 >> Using amp half precision backend\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.053, 'learning_rate': 5.2589073634204275e-08, 'epoch': 0.48}\n",
      "{'eval_loss': 0.37354376912117004, 'eval_accuracy': 0.9139908256880734, 'eval_runtime': 2.6528, 'eval_samples_per_second': 328.705, 'eval_steps_per_second': 5.277, 'epoch': 0.48}\n",
      "{'loss': 0.0525, 'learning_rate': 5.130641330166271e-09, 'epoch': 0.95}\n",
      "{'eval_loss': 0.37594789266586304, 'eval_accuracy': 0.9094036697247706, 'eval_runtime': 2.2829, 'eval_samples_per_second': 381.963, 'eval_steps_per_second': 6.132, 'epoch': 0.95}\n",
      "{'train_runtime': 490.8982, 'train_samples_per_second': 137.195, 'train_steps_per_second': 4.288, 'train_loss': 0.052978636136813854, 'epoch': 1.0}\n",
      "{'eval_loss': 0.37354376912117004, 'eval_accuracy': 0.9139908256880734, 'eval_runtime': 2.6102, 'eval_samples_per_second': 334.08, 'eval_steps_per_second': 5.364, 'epoch': 1.0}\n",
      "{'eval_loss': 0.37354376912117004, 'eval_accuracy': 0.9139908256880734, 'eval_runtime': 2.6102, 'eval_samples_per_second': 334.08, 'eval_steps_per_second': 5.364, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "args.learning_rate = 1e-7\n",
    "trainer = get_trainer(model_q)\n",
    "trainer.train()\n",
    "print(trainer.evaluate())\n",
    "model_q.save_pretrained(f\"{temp_dir}/model-torch-int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export a QDQ Pytorch model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28649/2328060802.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(v, dtype=torch.long, device=\"cuda\")\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  inputs, amax.item() / bound, 0,\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:291: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0])\n"
     ]
    }
   ],
   "source": [
    "data = encoded_dataset[\"train\"][1:3]\n",
    "input_torch = convert_tensor(data, output=\"torch\")\n",
    "convert_to_onnx(model_pytorch=model_q, output_path=f\"{temp_dir}/model-onnx-int8.onnx\", inputs_pytorch=input_torch, quantization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_q\n",
    "QATCalibrate.restore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build TensorRT engine from ONNX graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you have to choose right configuration options for your use case (= min, optimal and max shape), otherwise the results won't be that good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = trt.Runtime(trt_logger)\n",
    "engine = build_engine(\n",
    "    runtime=runtime,\n",
    "    onnx_file_path=f\"{temp_dir}/model-onnx-int8.onnx\",\n",
    "    logger=trt_logger,\n",
    "    min_shape=(1, max_seq_length),\n",
    "    optimal_shape=(batch_size, max_seq_length),\n",
    "    max_shape=(batch_size, max_seq_length),\n",
    "    workspace_size=10000 * 1024 * 1024,\n",
    "    fp16=True,\n",
    "    int8=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export TensorRT engine for Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_engine(engine=engine, engine_file_path=f\"{temp_dir}/model-tensorrt-int8.plan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Benchmarking TensorRT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_index=0\n",
    "stream = pycuda.driver.Stream()\n",
    "context = engine.create_execution_context()\n",
    "context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream.handle)\n",
    "input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmarker:\n",
    "    def __init__(self, name, pipeline, dataset=None) -> None:\n",
    "        self.name = name\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def measure_latency(self, input_data) -> dict:\n",
    "        latencies = list()\n",
    "        for _ in range(100):\n",
    "            self.pipeline(input_data)\n",
    "        \n",
    "        for _ in range(1000):\n",
    "            start_time = time.perf_counter()\n",
    "            self.pipeline(input_data)\n",
    "            end_time = time.perf_counter()\n",
    "            latencies.append((end_time - start_time)*1000)\n",
    "        \n",
    "        latencies = np.array(latencies)\n",
    "        return {\"average_latency\": np.mean(latencies), \"std_latency\": np.std(latencies)}\n",
    "    \n",
    "    def compute_accuracy(self, dataset=None) -> float:\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "        \n",
    "        predictions, labels = [], []\n",
    "\n",
    "        for sample in tqdm(self.dataset):\n",
    "            prediction = self.pipeline(sample[\"sentence\"])[0][\"label\"]\n",
    "            predictions.append(prediction)\n",
    "            labels.append(sample[\"label\"])\n",
    "        \n",
    "        return accuracy_score.compute(predictions=predictions, references=labels).get(\"accuracy\")\n",
    "    \n",
    "    def compute_size(self):\n",
    "        state_dict = self.pipeline.model.state_dict()\n",
    "        tmp_path = Path(\"model.pt\")\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
    "        tmp_path.unlink()\n",
    "        return size_mb\n",
    "    \n",
    "    def run_full_benchmark(self, input_data, dataset=None):\n",
    "        result = {\"name\": self.name}\n",
    "        result.update(self.measure_latency(input_data))\n",
    "        result[\"accuracy\"] = self.compute_accuracy(dataset) \n",
    "        result[\"size\"] = self.compute_size()\n",
    "        return result\n",
    "    \n",
    "    def print_results(self, benchmark_report):\n",
    "        print(f\"BENCHMARK REPORT\".center(40, \"-\"))\n",
    "        print(f\"Name {benchmark_report['name']}\")\n",
    "        print(f\"Latency: {benchmark_report['average_latency']:.2f} ms\")\n",
    "        print(f\"Accuracy on dataset: {benchmark_report['accuracy'] * 100:.2f}%\")\n",
    "        print(f\"Size: {benchmark_report['size']:.2f} MB\")\n",
    "        print(f\"\".center(40, \"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorRTBenchmarker(Benchmarker):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def compute_accuracy(self, dataset=None) -> float:\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "        \n",
    "        predictions, labels = [], []\n",
    "\n",
    "        for sample in tqdm(self.dataset):\n",
    "            prediction = self.pipeline(sample[\"sentence\"])[0][\"label\"]\n",
    "            predictions.append(prediction)\n",
    "            labels.append(sample[\"label\"])\n",
    "        \n",
    "        return accuracy_score.compute(predictions=predictions, references=labels).get(\"accuracy\")\n",
    "    \n",
    "    def compute_size(self):\n",
    "        save_engine(engine=engine, engine_file_path=\"model.plan\")\n",
    "        tmp_path = Path(\"model.plan\")\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
    "        tmp_path.unlink()\n",
    "        return size_mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorRTPipeline:\n",
    "    def __init__(self, engine, tokenizer):\n",
    "        self.engine = engine\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, data):\n",
    "        encoded_data = self.tokenizer([data], truncation=True, padding=\"max_length\", max_length=max_seq_length)\n",
    "        input_np = convert_tensor(data=encoded_data, output=\"np\")\n",
    "\n",
    "        logits = infer_tensorrt(\n",
    "            context=context,\n",
    "            host_inputs=input_np,\n",
    "            input_binding_idxs=input_binding_idxs,\n",
    "            output_binding_idxs=output_binding_idxs,\n",
    "            stream=stream,\n",
    "        )\n",
    "        probs = softmax(logits)\n",
    "        pred_idx = np.argmax(probs).item()\n",
    "        return [{\"label\": pred_idx, \"score\": probs.flatten()[pred_idx]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class PytorchPipeline:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, data):\n",
    "        input_torch = self.tokenizer(data, truncation=True, padding=\"max_length\", max_length=max_seq_length, return_tensors=\"pt\")\n",
    "        input_torch.to(\"cuda:0\")\n",
    "        output = baseline_model(**input_torch)\n",
    "        torch.cuda.synchronize()\n",
    "        probs = softmax(output.logits.cpu())\n",
    "        pred_idx = np.argmax(probs).item()\n",
    "        return [{\"label\": pred_idx, \"score\": probs.flatten()[pred_idx]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorRT baseline (INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 872/872 [00:01<00:00, 598.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------BENCHMARK REPORT------------\n",
      "Name model-trt-quant-int8\n",
      "Latency: 1.54 ms\n",
      "Accuracy on dataset: 90.71%\n",
      "Size: 323.34 MB\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21517/467476356.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "classifier = TensorRTPipeline(engine, tokenizer)\n",
    "benchmarker = TensorRTBenchmarker(f\"model-trt-quant-int8\", classifier, dataset=sentiment_dataset[\"validation\"])\n",
    "benchmark_report = benchmarker.run_full_benchmark(\"I like you!\")\n",
    "benchmarker.print_results(benchmark_report)\n",
    "benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch baseline (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 872/872 [00:05<00:00, 166.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------BENCHMARK REPORT------------\n",
      "Name model-torch-quant-fp32\n",
      "Latency: 5.84 ms\n",
      "Accuracy on dataset: 91.06%\n",
      "Size: 255.45 MB\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21517/494505071.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels).to(\"cuda:0\")\n",
    "baseline_model = baseline_model.cuda()\n",
    "baseline_model = baseline_model.eval()\n",
    "with torch.inference_mode(): \n",
    "    classifier = PytorchPipeline(model=baseline_model, tokenizer=tokenizer)\n",
    "    benchmarker = Benchmarker(f\"model-torch-quant-fp32\", classifier, sentiment_dataset[\"validation\"])\n",
    "    benchmark_report = benchmarker.run_full_benchmark(\"I like you!\")\n",
    "benchmarker.print_results(benchmark_report)\n",
    "benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch baseline (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 872/872 [00:04<00:00, 216.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------BENCHMARK REPORT------------\n",
      "Name model-torch-quant-fp16\n",
      "Latency: 4.81 ms\n",
      "Accuracy on dataset: 91.06%\n",
      "Size: 255.45 MB\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21517/2465702651.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "classifier = PytorchPipeline(model=baseline_model, tokenizer=tokenizer)\n",
    "with torch.inference_mode():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        benchmarker = Benchmarker(f\"model-torch-quant-fp16\", classifier, sentiment_dataset[\"validation\"])\n",
    "        benchmark_report = benchmarker.run_full_benchmark(\"I like you!\")\n",
    "benchmarker.print_results(benchmark_report)\n",
    "benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert PyTorch baseline (FP16) to ONNX graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21517/2328060802.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(v, dtype=torch.long, device=\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "data = encoded_dataset[\"train\"][1:3]\n",
    "input_torch = convert_tensor(data, output=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "baseline_model = baseline_model.cuda()\n",
    "convert_to_onnx(baseline_model, output_path=f\"{temp_dir}/model-onnx.onnx\", inputs_pytorch=input_torch, quantization=False)\n",
    "del baseline_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorRT baseline (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = trt.Runtime(trt_logger)\n",
    "profile_index=0\n",
    "engine = build_engine(\n",
    "    runtime=runtime,\n",
    "    onnx_file_path=f\"{temp_dir}/model-onnx.onnx\",\n",
    "    logger=trt_logger,\n",
    "    min_shape=(1, max_seq_length),\n",
    "    optimal_shape=(batch_size, max_seq_length),\n",
    "    max_shape=(batch_size, max_seq_length),\n",
    "    workspace_size=10000 * 1024 * 1024,\n",
    "    fp16=True,\n",
    "    int8=False,\n",
    ")\n",
    "save_engine(engine=engine, engine_file_path=f\"{temp_dir}/model-trt-quant-fp16.plan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_index = 0\n",
    "stream: Stream = pycuda.driver.Stream()\n",
    "context: IExecutionContext = engine.create_execution_context()\n",
    "context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream.handle)\n",
    "input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 872/872 [00:01<00:00, 510.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------BENCHMARK REPORT------------\n",
      "Name model-trt-fp16\n",
      "Latency: 1.73 ms\n",
      "Accuracy on dataset: 91.06%\n",
      "Size: 522.44 MB\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21517/2572588026.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "classifier = TensorRTPipeline(engine, tokenizer)\n",
    "benchmarker = TensorRTBenchmarker(f\"model-trt-fp16\", classifier, dataset=sentiment_dataset[\"validation\"])\n",
    "benchmark_report = benchmarker.run_full_benchmark(\"I like you!\")\n",
    "benchmarker.print_results(benchmark_report)\n",
    "benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONNX benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attention_heads, hidden_size = get_model_size(path=model_path)\n",
    "optimize_onnx(\n",
    "    onnx_path=f\"{temp_dir}/model-onnx.onnx\",\n",
    "    onnx_optim_model_path=f\"{temp_dir}/model-onnx-fp16.onnx\",\n",
    "    fp16=True,\n",
    "    use_cuda=True,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    hidden_size=hidden_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxPipeline:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, query):\n",
    "        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\n",
    "        inputs_onnx = {k: v.cpu().detach().numpy()\n",
    "                       for k, v in model_inputs.items()}\n",
    "        logits = self.model.run(None, inputs_onnx)[0][0, :]\n",
    "        probs = softmax(logits)\n",
    "        pred_idx = np.argmax(probs).item()\n",
    "        return [{\"label\": pred_idx, \"score\": probs[pred_idx]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxBenchmarker(Benchmarker):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def compute_size(self):\n",
    "        size_mb = Path(f\"{temp_dir}/{self.name}.onnx\").stat().st_size / (1024 * 1024)\n",
    "        return size_mb\n",
    "    \n",
    "    def compute_accuracy(self, dataset):\n",
    "        \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "        \n",
    "        predictions, labels = [], []\n",
    "\n",
    "        for sample in tqdm(self.dataset):\n",
    "            prediction = self.pipeline(sample[\"sentence\"])[0][\"label\"]\n",
    "            predictions.append(prediction)\n",
    "            labels.append(sample[\"label\"])\n",
    "        \n",
    "        return accuracy_score.compute(predictions=predictions, references=labels).get(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 872/872 [00:01<00:00, 611.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------BENCHMARK REPORT------------\n",
      "Name model-onnx-fp16\n",
      "Latency: 1.82 ms\n",
      "Accuracy on dataset: 91.06%\n",
      "Size: 132.79 MB\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_21517/3652221850.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "provider = \"CUDAExecutionProvider\"\n",
    "model = create_model_for_provider(path=f\"{temp_dir}/model-onnx-fp16.onnx\", provider_to_use=provider)\n",
    "classifier = OnnxPipeline(model, tokenizer)\n",
    "benchmarker = OnnxBenchmarker(\"model-onnx-fp16\", classifier, sentiment_dataset[\"validation\"])\n",
    "benchmark_report = benchmarker.run_full_benchmark(\"I like you!\")\n",
    "benchmarker.print_results(benchmark_report)\n",
    "benchmark_results_df = benchmark_results_df.append(benchmark_report, ignore_index=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = sns.barplot(x=\"average_latency\", y=\"name\", data=benchmark_results_df, order=benchmark_results_df.sort_values('average_latency', ascending=False)[\"name\"], orient=\"h\")\n",
    "graph.set_title(\"Comparison of the size of the models\")\n",
    "graph.set_ylabel(\"Model\")\n",
    "graph.set_xlabel(\"Average latency in ms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = sns.barplot(x=\"size\", y=\"name\", data=benchmark_results_df, order=benchmark_results_df.sort_values('size', ascending=False)[\"name\"], orient=\"h\")\n",
    "graph.set_title(\"Comparison of the size of the models\")\n",
    "graph.set_ylabel(\"Model\")\n",
    "graph.set_xlabel(\"Model size in MB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>average_latency</th>\n",
       "      <th>std_latency</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model-trt-quant-int8</td>\n",
       "      <td>1.535285</td>\n",
       "      <td>0.049556</td>\n",
       "      <td>0.90711</td>\n",
       "      <td>323.336419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model-torch-quant-fp32</td>\n",
       "      <td>5.839292</td>\n",
       "      <td>0.25408</td>\n",
       "      <td>0.91055</td>\n",
       "      <td>255.45128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model-torch-quant-fp16</td>\n",
       "      <td>4.813864</td>\n",
       "      <td>0.507173</td>\n",
       "      <td>0.91055</td>\n",
       "      <td>255.45128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model-trt-fp16</td>\n",
       "      <td>1.733514</td>\n",
       "      <td>0.262651</td>\n",
       "      <td>0.91055</td>\n",
       "      <td>522.438625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model-onnx-fp16</td>\n",
       "      <td>1.816036</td>\n",
       "      <td>0.175537</td>\n",
       "      <td>0.91055</td>\n",
       "      <td>132.787996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name average_latency std_latency accuracy        size\n",
       "0    model-trt-quant-int8        1.535285    0.049556  0.90711  323.336419\n",
       "1  model-torch-quant-fp32        5.839292     0.25408  0.91055   255.45128\n",
       "2  model-torch-quant-fp16        4.813864    0.507173  0.91055   255.45128\n",
       "3          model-trt-fp16        1.733514    0.262651  0.91055  522.438625\n",
       "4         model-onnx-fp16        1.816036    0.175537  0.91055  132.787996"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí® Conclusion: Techniques such as serving frameworks (e.g. ONNX, ONNX + TensorRT) and quantization can speed up transformer model inference a lot (more than 3.7x). The accuracy drop < 1%, so this is definitely a huge improvement. The TensorRT INT8 model is the fastest and ONNX fp16 model is the smallest."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86626b1ac732a796952688aab487e86bcc6df3c2d745fd32e029397d6b2f4272"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('gpu_optimized_transformer': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
